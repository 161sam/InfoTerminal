# InfoTerminal

[![CI](https://github.com/161sam/InfoTerminal/actions/workflows/ci.yml/badge.svg)](https://github.com/161sam/InfoTerminal/actions/workflows/ci.yml)
[![CodeQL](https://github.com/161sam/InfoTerminal/actions/workflows/codeql.yml/badge.svg)](https://github.com/161sam/InfoTerminal/actions/workflows/codeql.yml)

> **Open-Source Intelligence Plattform ‚Äì modular, sicher, erweiterbar.**
> ‚Äì Beyond Gotham ‚Äì mit offenen Technologien, AI/ML und Ethical Design.

[![Status](https://img.shields.io/badge/status-phase--2--wave--1-blue)](#)
[![License](https://img.shields.io/badge/license-Apache--2.0-green)](#)
[![K8s](https://img.shields.io/badge/kubernetes-ready-326ce5)](#)
[![Python](https://img.shields.io/badge/python-3.10%2B-yellow)](#)

---

## üß± Build-Stabilisierung (lokal)

- TypeScript-Audit: `npm run bs:ts-audit`
- Build-Validierung: `npm run bs:validate`
- Final-Check (Produktion): `npm run bs:final`

Details und Reports: `build-stabilization/README.md`

---

## üéØ Mission

**InfoTerminal** ist eine modulare OSINT/Intelligence-Plattform, die Nutzer:innen bef√§higt:

- Daten aus **News, Social Media, Open Data, Feeds, Sensoren** sicher und anonym zu sammeln
- **Graph-Analysen, NLP und Verifikations-Algorithmen** darauf anzuwenden
- Erkenntnisse in **Dossiers, Dashboards und Kollaborations-Workflows** bereitzustellen
- Vollst√§ndig mit **Security-Layer** (Incognito, Tor/VPN, Sandbox) und **Verification-Layer** (AI-Fact-Checking).

---

## üö¶ Phase 2 ‚Äì Wave¬†1 (Pakete¬†A & F)

- **Priorit√§t & Scope:** Graph-Analysen (Degree, Louvain, Shortest Path) und Dossier-Lite Export (Markdown/PDF) plus geteilte Notizen.
- **Artefakte:**
  - Planung & DoD: [`backlog/phase2/ITERATION-01_PLAN.md`](backlog/phase2/ITERATION-01_PLAN.md), [`backlog/phase2/WAVE1_DOD_CHECKLIST.md`](backlog/phase2/WAVE1_DOD_CHECKLIST.md)
  - Sequenz: [`backlog/phase2/PACKAGE_SEQUENCE.yaml`](backlog/phase2/PACKAGE_SEQUENCE.yaml)
  - Superset-Assets: `apps/superset/assets/datasets/graph_analytics_mvp.yaml`, `apps/superset/assets/charts/graph_degree_histogram.json`, `apps/superset/assets/dashboard/graph_analytics_mvp.json`
  - Grafana: `grafana/dashboards/graph-analytics-mvp.json`
- **Observability:** Neue Prometheus-Metriken `graph_analysis_queries_total`, `graph_analysis_duration_seconds_bucket`, `graph_subgraph_exports_total`, `dossier_exports_total`, `collab_notes_total`.
- **Gates:** Inventory/Policy, Health-Ready-Metrics, API/Docs und Smoke-E2E m√ºssen nach jedem Merge gr√ºn bleiben (`scripts/generate_inventory.py`, `test_infoterminal_v030_features.sh --suite graph-dossier`).

## üß™ 5-Minuten-Demo (offline, Wave¬†1‚Äì3)

> Ziel: In f√ºnf Minuten den End-to-End-Fluss **Search ‚Üí Graph-Analyse ‚Üí Dossier-Export ‚Üí NLP-Linking ‚Üí Geo-Overlay** demonstrieren ‚Äì komplett offline und reproduzierbar.

1. **Services & Seed-Daten starten**
   ```bash
   scripts/dev_up.sh graph dossier
   scripts/fixtures/load_graph_mini.sh
   ```
2. **Degree-Centrality pr√ºfen**
   ```bash
   curl -s "http://localhost:8612/graphs/analysis/degree?limit=10" |
     jq '.results[0:3]'
   ```
3. **Shortest-Path & Subgraph-Export**
   ```bash
   curl -s "http://localhost:8612/graphs/analysis/subgraph-export?center_id=demo-node&format=markdown" \
     -H 'Accept: application/json' > exports/demo-subgraph.json
   jq -r '.markdown' exports/demo-subgraph.json > exports/demo-subgraph.md
   ```
4. **Dossier-Lite (Markdown & PDF)**
   ```bash
   curl -s "http://localhost:8625/dossier/export" \
     -H 'Content-Type: application/json' \
     -d '{"case_id":"demo-case","format":"pdf","source":"graph","template":"brief"}' \
     -o exports/demo-dossier.pdf
   ```
5. **NLP Linking & Resolver ansto√üen**
   ```bash
   cat <<'JSON' > tmp/linking-input.json
   {
     "text": "Barack Obama met Donald Trump in Berlin to discuss cooperation with OpenAI.",
     "language": "en",
     "extract_entities": true,
     "extract_relations": true,
     "resolve_entities": true
   }
   JSON

   curl -s -X POST "http://localhost:8613/v1/documents/annotate" \
     -H 'Content-Type: application/json' \
     -d @tmp/linking-input.json > tmp/linking-result.json

   jq '.metadata.linking_status_counts' tmp/linking-result.json
   DOC_ID=$(jq -r '.doc_id' tmp/linking-result.json)
   curl -s -X POST "http://localhost:8613/v1/documents/${DOC_ID}/resolve" -H 'Content-Type: application/json' -d '{}'
   sleep 1
   curl -s "http://localhost:8613/v1/documents/${DOC_ID}" | jq '.entities[] | {text,label,resolution_status,resolution_target,resolution_score}'
   ```
6. **Geo-Overlay & Map Query pr√ºfen**
   ```bash
   curl -s "http://localhost:8612/geo/entities?south=40&west=-75&north=41&east=-73" | jq '.count'
   curl -s "http://localhost:8612/geo/entities?south=52&west=13&east=14&north=53" | jq '.entities[0]'
   ```
7. **Observability & Dashboards kontrollieren**
   - Prometheus: `/metrics` auf `graph-api`, `graph-views`, `collab-hub`, `plugin-runner`, `media-forensics` pr√ºfen (neue Counter/Histograms).
   - Prometheus: `doc_entities_resolver_runs_total`, `doc_entities_resolver_outcomes_total`, `doc_entities_linking_status_total`, `doc_entities_resolver_latency_seconds` beobachten.
   - Prometheus: `graph_geo_queries_total`, `geo_query_count`, `graph_geo_query_errors_total` auf der Graph-API.
   - Prometheus: `plugin_run_total{plugin="nmap"}`, `plugin_run_failures_total{plugin="nmap"}`, `video_frames_processed_total{pipeline="media_forensics"}` pr√ºfen.
   - Grafana: Dashboard **Graph Analytics MVP** (`grafana/dashboards/graph-analytics-mvp.json`).
   - Grafana: Panels **NLP Resolver Outcomes**, **Geo Query Volume**, **Plugin Execution Rate** und **Video Frames Processed Rate** (siehe `monitoring/grafana-dashboards/infoterminal-overview.json`).
   - Superset: Dashboard **Graph Analytics ‚Äì MVP** (`apps/superset/assets/dashboard/graph_analytics_mvp.json`).
   - Smoke: `scripts/smoke_graph_analysis.sh` verifiziert Degree, Louvain, Shortest Path & Subgraph Export.
8. **Plugin-Runner (nmap ‚Üí Graph ingest)**
   ```bash
   export PLUGINS_DIR="$(pwd)/plugins"
   export RESULTS_DIR="$(pwd)/tmp/plugin-results"
   export PLUGIN_TEST_MODE=1
   export IT_ENABLE_METRICS=1

   curl -s -X POST "http://localhost:8621/v1/plugins/nmap/execute" \
     -H 'Content-Type: application/json' \
     -d '{"parameters":{"target":"scan.example"},"output_format":"json"}' | jq '.'

   # Wait a moment and fetch the job payload including graph/search artefacts
   curl -s "http://localhost:8621/v1/jobs" | jq '.jobs[0] | {status,graph_entities}'
   cat tmp/plugin-results/graph_ingest/plugin_run_*.json | jq '.graph_entities[0]'
   ```
9. **Video-Pipeline (Frames ‚Üí Graph ingest)**
   ```bash
   export VIDEO_PIPELINE_ENABLED=1
   export MEDIA_GRAPH_FALLBACK_DIR="$(pwd)/tmp/video-graph"

   VIDEO_FILE=$(python - <<'PY'
import numpy as np
import tempfile
from PIL import Image

frames = []
for i in range(6):
    frame = np.zeros((64, 64, 3), dtype=np.uint8)
    frame[10 + i : 30 + i, 10:30] = 255
    frames.append(Image.fromarray(frame))

tmp = tempfile.NamedTemporaryFile(delete=False, suffix='.gif')
frames[0].save(tmp.name, save_all=True, append_images=frames[1:], duration=120, loop=0)
print(tmp.name)
PY
   )

   curl -s -X POST "http://localhost:8631/v1/videos/analyze" \
     -F "file=@${VIDEO_FILE}" -F frame_interval=1 -F min_area=50 | jq '.summary, .scenes[0]'

   cat tmp/video-graph/video_analysis_*.json | jq '.summary'
   ```

> üí° **Idempotent:** Wiederholtes Ausf√ºhren aktualisiert Exporte/Dashboards und √ºberschreibt Artefakte ohne Duplikate.

---

## ‚öôÔ∏è InfoTerminal CLI

Die **`infoterminal-cli`** (Typer-basiert, via `pipx` installierbar) ist der zentrale Zugang:

```bash
# Installation
pipx install infoterminal-cli

# Basisbefehle
it up          # Infrastruktur + Services starten
it down        # Stoppen
it status      # Status-√úbersicht
it logs        # Logs einsehen
it export      # Daten/Dossiers exportieren
it plugin run  # Plugins/Kali-Tools nutzen
it login       # OIDC/OAuth2 Login
```

Die CLI bietet:

- **Infra-Management:** Docker Compose + lokale Services
- **Abfragen:** Search, Graph, Views direkt aus dem Terminal
- **Exports:** GraphML, JSON, CSV, Dossier-Reports
- **Plugin-Runner:** Integration externer Tools (z. B. Kali Linux, nmap, exiftool)
- **Presets:** Profilbasierte Startkonfigurationen (`--preset journalism`, `--preset compliance`)

### Frontend via CLI
```bash
it fe dev     # Next.js dev
it fe build   # Production build
it fe test    # Vitest (dot reporter)
```

---

## üîÑ Flows & Pipelines

### 1. **Ingest (NiFi)**

- **RSS/API/File Watchers**: News, Dokumente, PDFs (OCR)
- **Social/Web Scraper**: Telegram, Reddit, Mastodon, Blogs
- **Streaming/Kafka**: Sensoren, CDR, Echtzeitfeeds
- **Video-Pipeline**: NiFi ‚Üí FFmpeg ‚Üí ML (Face/Object Detection)
- **Geospatial Enrichment**: Geocoding (OSM/Nominatim)

### 2. **Processing & Storage**

- **search-api (OpenSearch)**: Volltext, Embeddings, Ranking
- **graph-api (Neo4j)**: Entit√§ten, Relationen, Algorithmen
- **graph-views (Postgres)**: SQL-Views f√ºr BI (Superset, Grafana)
- **doc-entities (FastAPI)**: NER, Relation Extraction, Summarization (replaces legacy `nlp-service`)

### 3. **Verification-Layer**

- **Claim Extraction & Clustering**
- **Evidence Retrieval** (BM25 + Dense Rerank)
- **RTE/Stance Classification** (AI-Entailment)
- **Geo/Temporal Checks** (Mordecai, Heuristik)
- **Media Forensics** (pHash, EXIF, ELA, Reverse Search)
- **Veracity Score** + Active Learning + Review-UI

### 4. **Security-Layer**

- **Egress-Gateway** (Tor/VPN/Proxy, Kill-Switch, DNS-Sinkhole)
- **Ephemeral Filesystem** (Incognito Mode, Auto-Wipe)
- **Vault-Integration** (Secrets, Keys, WORM-Buckets)
- **Remote Browser Pool** (Fingerprints, Identity Profiles)
- **Plugin Sandbox** (gVisor/Kata/OPA Validation)

### 5. **Frontend**

- **/search**: Facetten, Ranking, Badges
- **/graphx**: Graph + Geo-Visualisierung, Algorithmen
- **/settings**: Endpoints, OAuth2/OIDC Config
- **Dossier-Builder**: PDF/MD-Reports, Vorlagen
- **Collaboration**: Shared Notes, CRDT, Audit-Logs

### 6. **Outputs**

- **Dossiers**: Templates f√ºr Legal, Disinfo, Financial, Crisis, Climate ‚Ä¶
- **Dashboards**: Superset (BI, Cross-Filter), Grafana (Logs/Metrics)
- **Presets**: Profile (Journalism, Agency, Research, Climate Researcher, Compliance Officer, Crisis Analyst, Disinfo Watchdog, Economic Analyst)

---

## üß© Module & Blueprints

- **Security-Blueprint** (Incognito, Vault, Sandbox, Logging)
- **Verification-Blueprint** (AI Fact-Checking, Veracity Layer)
- **Intelligence Packs (Beyond Gotham):**
  - v0.3 ‚Üí Legal, Disinformation, Supply-Chain
  - v0.5 ‚Üí Financial, Geopolitical, Humanitarian
  - v1.0 ‚Üí Climate, Technology, Terrorism, Health, AI-Ethics, Media-Forensics, Economic, Cultural

---

## üõ†Ô∏è Einsatzszenarien

- **Journalismus:** Fake-News erkennen, Narrative-Cluster analysieren, Beweisketten nachvollziehen
- **Beh√∂rden & Firmen:** Compliance-Verst√∂√üe pr√ºfen, Sanktionen √ºberwachen, Lieferketten simulieren
- **Forschung/NGOs:** Humanit√§re Krisen erkennen, Klimarisiken simulieren, Datenquellen fusionieren
- **Sicherheitsanalyse:** Terrornetzwerke, Propaganda, Finanzstr√∂me sichtbar machen
- **Ethik & Governance:** Bias in KI erkennen, Transparenz in Modellen durchsetzen

---

## ‚ùì Q\&A

### Warum InfoTerminal?

Weil es eine **offene, erweiterbare Alternative zu Palantir Gotham** bietet ‚Äì f√ºr Journalist\:innen, Beh√∂rden, NGOs und Forschende.

### Wof√ºr ist es gedacht?

- **Datenintegration & Analyse** aus heterogenen Quellen
- **Graph Intelligence** (Beziehungen, Netzwerke, Communities)
- **Verifikation & Fact-Checking** in Echtzeit
- **Simulation & Vorhersagen** (Supply Chains, Geopolitik, Klima)

### F√ºr wen?

- Journalist\:innen (Recherche, Fake-News Erkennung)
- Beh√∂rden & Sicherheitsorgane (Compliance, Risiko, Forensics)
- Forschung & NGOs (Krisen, Klima, Humanit√§r)
- Think Tanks & Unternehmen (Wirtschaft, Supply Chains, M√§rkte)

### Was kann InfoTerminal?

- Datenquellen anbinden (APIs, RSS, Social, Sensoren)
- NLP, Graph-Analysen, AI-Verifikation
- Incognito/Forensics-Modus f√ºr sichere Recherchen
- Reports/Dossiers automatisch generieren
- Presets & Playbooks f√ºr typische Nutzergruppen

### Was kann es (noch) nicht?

- Keine Echtzeit-Massen√ºberwachung (rechtlich & ethisch ausgeschlossen)
- Keine closed-source Abh√§ngigkeit ‚Äì Fokus auf Open-Source
- Kein Ersatz f√ºr menschliche Bewertung ‚Üí Human-in-the-Loop bleibt Pflicht
- Kein Data-Lake f√ºr beliebige Rohdaten (fokus auf kuratierten Ingest)

### Was unterscheidet InfoTerminal von Gotham?

- **Open Source, modular, f√∂deriert**
- **Ethical AI Toolkit** (Bias Checks, Transparenz)
- **Beyond Gotham Packs** (Climate, Cultural, Media-Forensics ‚Ä¶)
- **Plugin-Architektur** (Kali Tools, OSINT Scripts, APIs, SDKs)
- **Sustainability** (Green Hosting, effiziente ML Pipelines)

### Weitere Fragen

- **Wie starte ich?** ‚Üí siehe Quickstart unten
- **Wie erweitere ich?** ‚Üí eigene NiFi/n8n Flows, Plugins via YAML/SDK
- **Wie exportiere ich Ergebnisse?** ‚Üí CLI `it export` oder Dossier-Builder

---

## üë©‚Äçüíª Quickstart (Dev)

```bash
# 1. Infrastruktur starten (OpenSearch, Neo4j, Postgres, Prometheus, Grafana, ...)
docker compose --profile infra up -d

# 2. Lokale Services starten
./scripts/dev_up.sh

# 3. Frontend √∂ffnen
http://localhost:3411
```

---

## üïî 5-Minuten-Demo (Offline)

1. **Services starten** ‚Äì F√ºr den Wave-1-Durchstich reichen Neo4j, Graph-API, Collab-Hub, Superset und Grafana:
   ```bash
   # Basis-Services (Neo4j, Graph-API, Collab-Hub)
   docker compose up -d neo4j graph-api collab-hub

   # Superset (Profil aktivieren)
   docker compose --profile superset up -d superset

   # Grafana + Prometheus aus dem Observability-Stack
   docker compose -f docker-compose.yml -f docker-compose.observability.yml \
     --profile observability up -d grafana prometheus
   ```
2. **Beispielgraph laden** ‚Äì Seeds importieren (idempotent, √ºberschreibt vorhandene Demo-Daten nicht):
   ```bash
   python examples/py_cypher_demo.py --file examples/seeds/entities.json
   ```
3. **Graph-Analyse triggern** ‚Äì Degree-Centrality und Louvain Communities abrufen (zeigt auch Metrics-Z√§hler):
   ```bash
   curl -s "http://localhost:8612/graphs/analysis/degree?limit=10" | jq '.results'
   curl -s "http://localhost:8612/graphs/analysis/communities?limit=5" | jq '.communities'
   ```
4. **Dossier-Hook verwenden** ‚Äì Subgraph-Export anfordern und als Markdown speichern:
   ```bash
   curl -s "http://localhost:8612/graphs/analysis/subgraph-export?center_id=demo-node&format=markdown" \
     -H 'Accept: application/json' > exports/demo-subgraph.json
   jq -r '.markdown' exports/demo-subgraph.json > exports/demo-subgraph.md
   ```
5. **Dossier-Lite erzeugen** ‚Äì Export als PDF/MD via Collab-Hub (Feature-Flag aktivieren falls n√∂tig):
   ```bash
   curl -s -X POST "http://localhost:8625/dossier/export" \
     -H 'Content-Type: application/json' \
     -d '{"case_id":"demo","format":"pdf","source":"graph"}' \
     -o exports/demo-dossier.pdf
   ```
6. **Dashboards pr√ºfen** ‚Äì
   - Grafana ‚Üí Dashboard ‚ÄûGraph Analytics MVP‚Äú: Query-Rate, Dauer p95, Subgraph-Exporte.
   - Superset ‚Üí Dashboard ‚Äûgraph_analytics_mvp‚Äú: Degree-Histogramm, Community-Count.

Die Schritte sind wiederholbar und funktionieren ohne Internet-Zugriff. Alle Exportbefehle sind idempotent ‚Äì bestehende Artefakte werden √ºberschrieben statt dupliziert.

---

## üßπ Dev Hygiene

**Lint & Format**

```bash
make gv.venv    # einmalig
make lint       # strikt (CI-√§hnlich)
make format     # auto-fix, tolerant
```

**Tipps**

- Pre-commit installiert/aktualisiert sich automatisch bei `make lint`. Einmalige manuelle Installation ist nicht n√∂tig.
- Falls `make lint` scheitert: `make format` ausf√ºhren und erneut `make lint` starten.

**Helm/Templated YAML**
Prettier √ºberspringt Helm-Templates via `.prettierignore` und Hook-`exclude`. Falls neue Charts/Templates hinzukommen, bitte Pfade in `.prettierignore` erg√§nzen.

**Lint/Format**

```bash
make gv.venv   # einmalig
make format    # auto-fix (tolerant)
make lint      # strikt
```

**Safe Prettier**
F√ºr schnelle, stabile Formatierung ohne Templating-/Ops-Dateien:
```bash
make fmt.safe      # schreibt nur kuratierte Pfade
make lint.safe     # checkt nur kuratierte Pfade
```

Die kuratierten Pfade liegen in `scripts/prettier_safe.list`. Bei Bedarf erweitern.

---

## üìú License

Apache 2.0 ‚Äì frei nutzbar, erweiterbar, beitragbar.

---

## ü§ù Contributing

Pull Requests willkommen.
Bitte beachte: kleine, deterministische PRs mit Tests & Docs.
